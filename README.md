# ğŸ›ï¸ Retail E-commerce Analytics Pipeline  
**Apache Airflow 3.0 + PostgreSQL + AWS (S3, Glue)**

This project builds a robust end-to-end data pipeline for processing and analyzing e-commerce events. Built using modern Data Engineering best practices with Airflow assets, incremental processing, PostgreSQL, and AWS integration.

---

## ğŸ“ Folder Structure

```

.
â”œâ”€â”€ aws\_resources/          # CDK-generated AWS infrastructure (S3, Glue Crawler)
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ retail\_pipeline\_assets.py   # Airflow DAG for staging â†’ fact/dim
â”‚   â””â”€â”€ retail\_pipeline.py          # Airflow DAG for aggregates + Glue Crawler
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ table\_creation.sql  # PostgreSQL DDL for staging + mart schema
â”œâ”€â”€ architecture/
â”‚   â””â”€â”€ Airflow-Retail-ETL.drawio.png  # Architecture diagram
â”œâ”€â”€ docker-compose.yaml     # Local Airflow deployment
â””â”€â”€ .venv/                  # Python virtual environment (excluded from git)

````

---

## âš™ï¸ Tech Stack

| Component              | Purpose                                     |
|------------------------|---------------------------------------------|
| Apache Airflow 3.0     | Orchestration using asset-based DAGs       |
| PostgreSQL             | Staging and dimensional modeling            |
| AWS S3                 | Raw and aggregated data storage             |
| AWS Glue Crawler       | Schema registry for Parquet marts           |
| Docker Compose         | Local Airflow setup                         |
| DuckDB + Pandas        | Optional in-memory data transformations     |

---

## ğŸ“Œ Pipeline Overview

### ğŸ”¹ DAG 1: `retail_pipeline_assets.py`

- Reads daily raw event CSVs from `s3://airflow-retail-stage/`
- Loads into:
  - `staging_events`
  - Dimension tables: `dim_product`, `dim_user`, etc.
  - Fact table: `fact_events`
- All loads are incremental using execution date.

### ğŸ”¹ DAG 2: `retail_pipeline.py`

- Computes daily KPIs:
  - Total revenue
  - Active users
  - Funnel conversion
- Stores aggregates in Parquet under `s3://airflow-retail-mart/aggregates/...`
- Triggers Glue Crawler to update the AWS Glue Data Catalog

---

## ğŸ–¼ï¸ Architecture Diagram

![Retail Pipeline Architecture](architecture/Airflow-Retail-ETL.drawio.png)

---

## ğŸ“Š Key Business Metrics

- ğŸ›’ Total daily revenue
- ğŸ‘¤ Daily active users
- ğŸ”„ Funnel: Views â†’ Carts â†’ Purchases
- ğŸ† Top brands and categories

---

## ğŸš€ Quick Start (Local)

```bash
# 1. Clone repo
git clone https://github.com/yourusername/retail-pipeline.git
cd retail-pipeline

# 2. Start Airflow locally
docker-compose up --build

# 3. Open Airflow UI
# http://localhost:8080

# 4. Create tables (in PostgreSQL)
psql -h localhost -U airflow -d airflow -f scripts/table_creation.sql
````

---

## ğŸ“¦ S3 Structure

* **Raw data**
  `s3://airflow-retail-stage/month/YYYY-MM-DD/event.csv`

* **Aggregated output**
  `s3://airflow-retail-mart/aggregates/<metric>/dt=YYYY-MM-DD/...`

---

## ğŸ”„ AWS Integration

* Airflow connects to AWS via the `aws` connection.
* CDK script under `aws_resources/` provisions:

  * S3 buckets
  * Glue Crawler
* Crawler is triggered in the `retail_pipeline` DAG

---

## âœ… To-Do

* [ ] Add tests and CI workflow
* [ ] Add Athena queries over the parquet data
* [ ] Add Great Expectations or Data Quality checks
* [ ] Add Redshift Spectrum integration

---

## ğŸªª License

Licensed under the [MIT License](LICENSE).
